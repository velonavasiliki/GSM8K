# Imitation Learning for Mathematical Reasoning: GSM8K with Qwen2.5-1.5B

This notebook demonstrates **imitation learning** (behavior cloning) for improving mathematical reasoning in small language models. We fine-tune Qwen2.5-1.5B (1.5 billion parameters) on the GSM8K dataset using synthetic chain-of-thought reasoning generated by three powerful teacher models:

- **GPT-4o-mini** (OpenAI)
- **DeepSeek-chat** (DeepSeek)
- **Gemini Flash** (Google)

### Results

| Approach | Accuracy | vs. Base Model |
|----------|----------|----------------|
| Base Model (no tuning) | 60% | - |
| Fine-tuned on GSM8K | 63% | +3% |
| **GPT-4o-mini imitation** | **67%** | **+7%** |
| DeepSeek imitation | 66% | +6% |
| Gemini imitation | 63% | +3% |

### Model & Dataset
- **Student Model:** Qwen/Qwen2.5-1.5B-Instruct (1.5B parameters)
- **Teacher Models:** GPT-4o-mini, DeepSeek-chat, Gemini-flash-latest
- **Dataset:** GSM8K (grade school math word problems)
- **Training/Test Size:** 100/100 examples. TBD: training data size to be increased (1000+ examples).

* **Teacher quality matters:** GPT-4o-mini and DeepSeek both provided strong improvements (6-7%), suggesting their reasoning patterns transfer well to Qwen, even with just 100 training examples. 

## Structure

1. Loading and preparing the GSM8K dataset
2. Generating synthetic CoT reasoning from teacher models
3. Fine-tuning with LoRA and quantization
4. Evaluating on held-out test set
5. Comparing multiple teacher models

## References

- **Dataset:** [GSM8K](https://github.com/openai/grade-school-math) - Grade School Math 8K problems
- **Model:** [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
